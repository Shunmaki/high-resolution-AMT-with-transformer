{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2daaea4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchlibrosa'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-29e68cd09a28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpectrogram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLogmelFilterBank\u001b[0m \u001b[0;31m# use 'magphase' to extract phase informatinon with magnitude\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchlibrosa'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import audioread\n",
    "\n",
    "\n",
    "from torchlibrosa.stft import Spectrogram, LogmelFilterBank # use 'magphase' to extract phase informatinon with magnitude\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4aa33493",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# initialize and conv block\n",
    "def init_layer(layer):\n",
    "    \"\"\"Initialize a Linear or Convolutional layer. \"\"\"\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    " \n",
    "    if hasattr(layer, 'bias'):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "            \n",
    "    \n",
    "def init_bn(bn):\n",
    "    \"\"\"Initialize a Batchnorm layer. \"\"\"\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.)\n",
    "\n",
    "\n",
    "def init_gru(rnn):\n",
    "    \"\"\"Initialize a GRU layer. \"\"\"\n",
    "    \n",
    "    def _concat_init(tensor, init_funcs):\n",
    "        (length, fan_out) = tensor.shape\n",
    "        fan_in = length // len(init_funcs)\n",
    "    \n",
    "        for (i, init_func) in enumerate(init_funcs):\n",
    "            init_func(tensor[i * fan_in : (i + 1) * fan_in, :])\n",
    "        \n",
    "    def _inner_uniform(tensor):\n",
    "        fan_in = nn.init._calculate_correct_fan(tensor, 'fan_in')\n",
    "        nn.init.uniform_(tensor, -math.sqrt(3 / fan_in), math.sqrt(3 / fan_in))\n",
    "    \n",
    "    for i in range(rnn.num_layers):\n",
    "        _concat_init(\n",
    "            getattr(rnn, 'weight_ih_l{}'.format(i)),\n",
    "            [_inner_uniform, _inner_uniform, _inner_uniform]\n",
    "        )\n",
    "        torch.nn.init.constant_(getattr(rnn, 'bias_ih_l{}'.format(i)), 0)\n",
    "\n",
    "        _concat_init(\n",
    "            getattr(rnn, 'weight_hh_l{}'.format(i)),\n",
    "            [_inner_uniform, _inner_uniform, nn.init.orthogonal_]\n",
    "        )\n",
    "        torch.nn.init.constant_(getattr(rnn, 'bias_hh_l{}'.format(i)), 0)\n",
    "        \n",
    "def init_ln(module):\n",
    "    if isinstance(module, nn.Embedding):\n",
    "        module.weight.data.normal_(mean=0.0, std=1.0)\n",
    "        if module.padding_idx is not None:\n",
    "            module.weight.data[module.padding_idx].zero_()\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        module.bias.data.zero_()\n",
    "        module.weight.data.fill_(1.0)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x.transpose(0,1)\n",
    "        print(x.shape)\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        x = x.transpose(0,1)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model=768, nhead=8):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead) #,batch_first=True) torch==1.8.0ではbatch_firstが使えない、、\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=4)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.transpose(0,1)\n",
    "        output = self.transformer_encoder(inputs)\n",
    "        output = output.transpose(0,1)\n",
    "        return output\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, momentum):\n",
    "        \n",
    "        super(ConvBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=(3, 3), stride=(1, 1),\n",
    "                              padding=(1, 1), bias=False)\n",
    "                              \n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=(3, 3), stride=(1, 1),\n",
    "                              padding=(1, 1), bias=False)\n",
    "                              \n",
    "        self.bn1 = nn.BatchNorm2d(out_channels, momentum)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels, momentum)\n",
    "\n",
    "        self.init_weight()\n",
    "        \n",
    "    def init_weight(self):\n",
    "        init_layer(self.conv1)\n",
    "        init_layer(self.conv2)\n",
    "        init_bn(self.bn1)\n",
    "        init_bn(self.bn2)\n",
    "\n",
    "        \n",
    "    def forward(self, input, pool_size=(2, 2), pool_type='avg'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          input: (batch_size, in_channels, time_steps, freq_bins)\n",
    "        Outputs:\n",
    "          output: (batch_size, out_channels, classes_num)\n",
    "        \"\"\"\n",
    "\n",
    "        x = F.relu_(self.bn1(self.conv1(input)))\n",
    "        #print(x)\n",
    "        x = F.relu_(self.bn2(self.conv2(x)))\n",
    "        \n",
    "        if pool_type == 'avg':\n",
    "            x = F.avg_pool2d(x, kernel_size=pool_size)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "# アコースティックモデル\n",
    "class AcousticModelCRnn8Dropout(nn.Module):\n",
    "    def __init__(self, classes_num, midfeat, momentum):\n",
    "        super(AcousticModelCRnn8Dropout, self).__init__()\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=48, momentum=momentum)\n",
    "        self.conv_block2 = ConvBlock(in_channels=48, out_channels=64, momentum=momentum)\n",
    "        self.conv_block3 = ConvBlock(in_channels=64, out_channels=96, momentum=momentum)\n",
    "        self.conv_block4 = ConvBlock(in_channels=96, out_channels=128, momentum=momentum)\n",
    "\n",
    "        self.fc5 = nn.Linear(midfeat, 768, bias=False)\n",
    "        self.bn5 = nn.BatchNorm1d(768, momentum=momentum)\n",
    "\n",
    "        self.gru = nn.GRU(input_size=768, hidden_size=256, num_layers=2, \n",
    "            bias=True, batch_first=True, dropout=0., bidirectional=True)\n",
    "\n",
    "        self.fc = nn.Linear(512, classes_num, bias=True)\n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_layer(self.fc5)\n",
    "        init_bn(self.bn5)\n",
    "        init_gru(self.gru)\n",
    "        init_layer(self.fc)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          input: (batch_size, channels_num, time_steps, freq_bins)\n",
    "        Outputs:\n",
    "          output: (batch_size, time_steps, classes_num)\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.conv_block1(input, pool_size=(1, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block2(x, pool_size=(1, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block3(x, pool_size=(1, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block4(x, pool_size=(1, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "\n",
    "        x = x.transpose(1, 2).flatten(2)\n",
    "        x = F.relu(self.bn5(self.fc5(x).transpose(1, 2)).transpose(1, 2))\n",
    "        x = F.dropout(x, p=0.5, training=self.training, inplace=True)\n",
    "        \n",
    "        (x, _) = self.gru(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training, inplace=False)\n",
    "        output = torch.sigmoid(self.fc(x))\n",
    "        return output \n",
    "    \n",
    "# アコースティックモデル(Transformer バージョン)\n",
    "\n",
    "# アコースティックモデル\n",
    "class AcousticModelTransformer(nn.Module):\n",
    "    def __init__(self, classes_num, midfeat, momentum):\n",
    "        super(AcousticModelTransformer, self).__init__()\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=48, momentum=momentum)\n",
    "        self.conv_block2 = ConvBlock(in_channels=48, out_channels=64, momentum=momentum)\n",
    "        self.conv_block3 = ConvBlock(in_channels=64, out_channels=96, momentum=momentum)\n",
    "        self.conv_block4 = ConvBlock(in_channels=96, out_channels=128, momentum=momentum)\n",
    "\n",
    "        self.fc5 = nn.Linear(midfeat, 768, bias=False)\n",
    "        self.bn5 = nn.BatchNorm1d(768, momentum=momentum)\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(768)\n",
    "        self.pe = PositionalEncoding(d_model=768, dropout=0.1)\n",
    "        \n",
    "        self.encoder_layer = TransformerEncoder()\n",
    "\n",
    "        self.fc1 = nn.Linear(768, 512, bias=True)\n",
    "        self.fc2 = nn.Linear(512, 256, bias=True)\n",
    "        self.fc3 = nn.Linear(256, classes_num, bias=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_layer(self.fc5)\n",
    "        init_bn(self.bn5)\n",
    "        init_layer(self.fc1)\n",
    "        init_layer(self.fc2)\n",
    "        init_layer(self.fc3)\n",
    "        init_ln(self.ln1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          input: (batch_size, channels_num, time_steps, freq_bins)\n",
    "        Outputs:\n",
    "          output: (batch_size, time_steps, classes_num)\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.conv_block1(input, pool_size=(1, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block2(x, pool_size=(1, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block3(x, pool_size=(1, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block4(x, pool_size=(1, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training) # [batch_size, 128, 1001, 14]\n",
    "        \n",
    "        x = x.transpose(1, 2).flatten(2)  # [batch_size, 1001, 1792]\n",
    "\n",
    "        x = F.relu(self.fc5(x)) # [batch_size, 1001, 768]\n",
    "        x = self.ln1(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training, inplace=True)  # [batch_size, 1001, 768]\n",
    "        \n",
    "        print(x.shape)\n",
    "        x = self.pe(x) # [batch_soze, 1001, 768]\n",
    "        print(x.shape)\n",
    "        \n",
    "        x = self.encoder_layer(x)  # [batch_size, 1001, 768]\n",
    "        x = F.dropout(x, p=0.5, training=self.training, inplace=False)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        output = torch.sigmoid(self.fc3(x))\n",
    "        \n",
    "        # この下にTransformerのEncoder部分(4層)を書く(to do)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b30b7964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1001, 768])\n",
      "torch.Size([1001, 10, 768])\n",
      "torch.Size([10, 1001, 768])\n"
     ]
    }
   ],
   "source": [
    "inputs_batch = torch.ones(10, 1, 1001, 229)\n",
    "NET = AcousticModelTransformer(classes_num=88, midfeat=1792, momentum=0.1)\n",
    "outputs = NET(inputs_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4053826c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1001, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e827569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.0+cu111'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be830c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd74a93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.8.0+cu111\n",
      "  Downloading https://download.pytorch.org/whl/cu111/torch-1.8.0%2Bcu111-cp36-cp36m-linux_x86_64.whl (1982.2 MB)\n",
      "     |################################| 1982.2 MB 10 kB/s              #######                   | 805.8 MB 3.4 MB/s eta 0:05:42 ########                   | 816.1 MB 23.7 MB/s eta 0:00:50#########                  | 923.7 MB 821 kB/s eta 0:21:28 ###########                | 1010.6 MB 941 kB/s eta 0:17:12               | 1043.4 MB 877 kB/s eta 0:17:50###########                | 1048.8 MB 877 kB/s eta 0:17:44############               | 1073.2 MB 907 kB/s eta 0:16:42############               | 1086.1 MB 973 kB/s eta 0:15:21#              | 1140.5 MB 910 kB/s eta 0:15:25##############             | 1207.2 MB 972 kB/s eta 0:13:18##############             | 1237.0 MB 911 kB/s eta 0:13:38###############            | 1256.0 MB 911 kB/s eta 0:13:17###            | 1264.1 MB 1.0 MB/s eta 0:11:32###############            | 1268.9 MB 1.0 MB/s eta 0:11:28###############            | 1294.6 MB 872 kB/s eta 0:13:09###############            | 1300.5 MB 897 kB/s eta 0:12:40################           | 1306.5 MB 897 kB/s eta 0:12:33##################         | 1428.4 MB 2.4 MB/s eta 0:03:56###################        | 1491.6 MB 1.1 MB/s eta 0:07:38###################        | 1502.3 MB 1.1 MB/s eta 0:07:28###################        | 1510.4 MB 1.7 MB/s eta 0:04:32####################       | 1551.7 MB 3.2 MB/s eta 0:02:13####################       | 1560.0 MB 3.2 MB/s eta 0:02:10####################       | 1568.5 MB 3.2 MB/s eta 0:02:08####################       | 1579.7 MB 2.9 MB/s eta 0:02:20####################       | 1601.1 MB 3.4 MB/s eta 0:01:53####################       | 1607.3 MB 852 kB/s eta 0:07:21#####################      | 1649.7 MB 890 kB/s eta 0:06:14#####################      | 1656.8 MB 1.1 MB/s eta 0:04:58#####################      | 1668.6 MB 1.1 MB/s eta 0:04:49######################     | 1673.4 MB 1.1 MB/s eta 0:04:45######################     | 1683.0 MB 1.1 MB/s eta 0:04:36######################     | 1690.2 MB 1.1 MB/s eta 0:04:17######################     | 1702.3 MB 1.2 MB/s eta 0:03:53######################     | 1707.2 MB 1.2 MB/s eta 0:03:49######################     | 1714.5 MB 1.2 MB/s eta 0:03:43######################     | 1721.8 MB 1.2 MB/s eta 0:03:32######################     | 1729.1 MB 1.2 MB/s eta 0:03:26#######################    | 1736.4 MB 1.3 MB/s eta 0:03:14#######################    | 1743.7 MB 1.3 MB/s eta 0:03:08#######################    | 1751.0 MB 1.3 MB/s eta 0:03:02#######################    | 1758.3 MB 1.3 MB/s eta 0:02:55#######################    | 1765.6 MB 1.3 MB/s eta 0:02:49#######################    | 1772.9 MB 1.3 MB/s eta 0:02:43#######################    | 1780.2 MB 1.3 MB/s eta 0:02:37#######################    | 1792.4 MB 1.3 MB/s eta 0:02:26########################   | 1799.8 MB 1.3 MB/s eta 0:02:21########################## | 1930.1 MB 1.7 MB/s eta 0:00:32########################## | 1936.3 MB 1.3 MB/s eta 0:00:36########################## | 1942.5 MB 1.3 MB/s eta 0:00:31########################## | 1954.7 MB 1.3 MB/s eta 0:00:22MB/s eta 0:00:01B/s eta 0:00:01 \n",
      "\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch==1.8.0+cu111) (0.8)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.8.0+cu111) (1.16.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.8.0+cu111) (4.1.1)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.8.0\n",
      "    Uninstalling torch-1.8.0:\n",
      "      Successfully uninstalled torch-1.8.0\n",
      "Successfully installed torch-1.8.0+cu111\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.8.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b20349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import audioread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37748425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.6'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audioread.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06779674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting audioread==3.0.0\n",
      "  Downloading audioread-3.0.0.tar.gz (377 kB)\n",
      "     |################################| 377 kB 11.8 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: audioread\n",
      "  Building wheel for audioread (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for audioread: filename=audioread-3.0.0-py3-none-any.whl size=23693 sha256=103938a40bc02d57106cbe6547f65d28d2281ea313e580d86fc620dfb9e7c75c\n",
      "  Stored in directory: /root/.cache/pip/wheels/00/be/fc/a93c5810787b4f37cd2a5336f8291235efbf0da00bb04add66\n",
      "Successfully built audioread\n",
      "Installing collected packages: audioread\n",
      "  Attempting uninstall: audioread\n",
      "    Found existing installation: audioread 2.1.6\n",
      "    Uninstalling audioread-2.1.6:\n",
      "      Successfully uninstalled audioread-2.1.6\n",
      "Successfully installed audioread-3.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install audioread==3.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2472b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
