{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2daaea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import audioread\n",
    "\n",
    "\n",
    "from torchlibrosa.stft import Spectrogram, LogmelFilterBank # use 'magphase' to extract phase informatinon with magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "287ec179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.8.0+cu111\n",
      "  Downloading https://download.pytorch.org/whl/cu111/torch-1.8.0%2Bcu111-cp36-cp36m-linux_x86_64.whl (1982.2 MB)\n",
      "     |################################| 1982.2 MB 12 kB/s               \n",
      "\u001b[?25hCollecting torchvision==0.9.0+cu111\n",
      "  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.9.0%2Bcu111-cp36-cp36m-linux_x86_64.whl (17.6 MB)\n",
      "     |################################| 17.6 MB 31.2 MB/s            \n",
      "\u001b[?25hCollecting torchaudio==0.8.0\n",
      "  Downloading torchaudio-0.8.0-cp36-cp36m-manylinux1_x86_64.whl (1.9 MB)\n",
      "     |################################| 1.9 MB 12.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.8.0+cu111) (4.1.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.8.0+cu111) (1.16.2)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch==1.8.0+cu111) (0.8)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.9.0+cu111) (6.0.0)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.8.0\n",
      "    Uninstalling torch-1.8.0:\n",
      "      Successfully uninstalled torch-1.8.0\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.9.0\n",
      "    Uninstalling torchvision-0.9.0:\n",
      "      Successfully uninstalled torchvision-0.9.0\n",
      "Successfully installed torch-1.8.0+cu111 torchaudio-0.8.0 torchvision-0.9.0+cu111\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8d27d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.0+cu111'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdfb403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "m = nn.Conv1d(16, 33, 3, stride=2)\n",
    "m=m.to('cuda')\n",
    "input = torch.randn(20, 16, 50)\n",
    "input=input.to('cuda')\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f1e9a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "565f5dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize and conv block\n",
    "def init_layer(layer):\n",
    "    \"\"\"Initialize a Linear or Convolutional layer. \"\"\"\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    " \n",
    "    if hasattr(layer, 'bias'):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "            \n",
    "    \n",
    "def init_bn(bn):\n",
    "    \"\"\"Initialize a Batchnorm layer. \"\"\"\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.)\n",
    "\n",
    "\n",
    "def init_gru(rnn):\n",
    "    \"\"\"Initialize a GRU layer. \"\"\"\n",
    "    \n",
    "    def _concat_init(tensor, init_funcs):\n",
    "        (length, fan_out) = tensor.shape\n",
    "        fan_in = length // len(init_funcs)\n",
    "    \n",
    "        for (i, init_func) in enumerate(init_funcs):\n",
    "            init_func(tensor[i * fan_in : (i + 1) * fan_in, :])\n",
    "        \n",
    "    def _inner_uniform(tensor):\n",
    "        fan_in = nn.init._calculate_correct_fan(tensor, 'fan_in')\n",
    "        nn.init.uniform_(tensor, -math.sqrt(3 / fan_in), math.sqrt(3 / fan_in))\n",
    "    \n",
    "    for i in range(rnn.num_layers):\n",
    "        _concat_init(\n",
    "            getattr(rnn, 'weight_ih_l{}'.format(i)),\n",
    "            [_inner_uniform, _inner_uniform, _inner_uniform]\n",
    "        )\n",
    "        torch.nn.init.constant_(getattr(rnn, 'bias_ih_l{}'.format(i)), 0)\n",
    "\n",
    "        _concat_init(\n",
    "            getattr(rnn, 'weight_hh_l{}'.format(i)),\n",
    "            [_inner_uniform, _inner_uniform, nn.init.orthogonal_]\n",
    "        )\n",
    "        torch.nn.init.constant_(getattr(rnn, 'bias_hh_l{}'.format(i)), 0)\n",
    "    \n",
    "def init_ln(module):\n",
    "    if isinstance(module, nn.Embedding):\n",
    "        module.weight.data.normal_(mean=0.0, std=1.0)\n",
    "        if module.padding_idx is not None:\n",
    "            module.weight.data[module.padding_idx].zero_()\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        module.bias.data.zero_()\n",
    "        module.weight.data.fill_(1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7edf7b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, momentum):\n",
    "        \n",
    "        super(ConvBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=(3, 3), stride=(1, 1),\n",
    "                              padding=(1, 1), bias=False)\n",
    "                              \n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=(3, 3), stride=(1, 1),\n",
    "                              padding=(1, 1), bias=False)\n",
    "                              \n",
    "        self.bn1 = nn.BatchNorm2d(out_channels, momentum)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels, momentum)\n",
    "\n",
    "        self.init_weight()\n",
    "        \n",
    "    def init_weight(self):\n",
    "        init_layer(self.conv1)\n",
    "        init_layer(self.conv2)\n",
    "        init_bn(self.bn1)\n",
    "        init_bn(self.bn2)\n",
    "\n",
    "        \n",
    "    def forward(self, input, pool_size=(2, 2), pool_type='avg'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          input: (batch_size, in_channels, time_steps, freq_bins)\n",
    "        Outputs:\n",
    "          output: (batch_size, out_channels, classes_num)\n",
    "        \"\"\"\n",
    "\n",
    "        x = F.relu_(self.bn1(self.conv1(input)))\n",
    "        #print(x)\n",
    "        x = F.relu_(self.bn2(self.conv2(x)))\n",
    "        \n",
    "        if pool_type == 'avg':\n",
    "            x = F.avg_pool2d(x, kernel_size=pool_size)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2ca7ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model=768, nhead=8):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=4)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        inputs = inputs.transpose(0,1)\n",
    "        output = self.transformer_encoder(inputs)\n",
    "        output = output.transpose(0,1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f492abd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# アコースティックモデル\n",
    "class AcousticModelCRnn8Dropout(nn.Module):\n",
    "    def __init__(self, classes_num, midfeat, momentum):\n",
    "        super(AcousticModelCRnn8Dropout, self).__init__()\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=48, momentum=momentum)\n",
    "        self.conv_block2 = ConvBlock(in_channels=48, out_channels=64, momentum=momentum)\n",
    "        self.conv_block3 = ConvBlock(in_channels=64, out_channels=96, momentum=momentum)\n",
    "        self.conv_block4 = ConvBlock(in_channels=96, out_channels=128, momentum=momentum)\n",
    "\n",
    "        self.fc5 = nn.Linear(midfeat, 768, bias=False)\n",
    "        self.bn5 = nn.BatchNorm1d(768, momentum=momentum)\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(768)\n",
    "        self.pe = PositionalEncoding(d_model=768, dropout=0.1)\n",
    "\n",
    "        self.fc = nn.Linear(768, classes_num, bias=True)\n",
    "        \n",
    "        self.encoder_layer = TransformerEncoder()\n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_layer(self.fc5)\n",
    "        init_bn(self.bn5)\n",
    "        init_layer(self.fc)\n",
    "        init_ln(self.ln1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          input: (batch_size, channels_num, time_steps, freq_bins)\n",
    "        Outputs:\n",
    "          output: (batch_size, time_steps, classes_num)\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.conv_block1(input, pool_size=(1, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block2(x, pool_size=(1, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block3(x, pool_size=(1, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block4(x, pool_size=(1, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training) # [batch_size, 128, 1001, 14]\n",
    "        \n",
    "        x = x.transpose(1, 2).flatten(2)  # [batch_size, 1001, 1792]\n",
    "\n",
    "        x = F.relu(self.fc5(x)) # [batch_size, 1001, 768]\n",
    "        x = self.ln1(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training, inplace=True)  # [batch_size, 1001, 768]\n",
    "        x = self.pe(x) # [batch_soze, 1001, 768]\n",
    "        \n",
    "        x = self.encoder_layer(x)  # [batch_size, 1001, 768]\n",
    "        x = F.dropout(x, p=0.5, training=self.training, inplace=False)\n",
    "        \n",
    "        output = torch.sigmoid(self.fc(x))\n",
    "        \n",
    "        # この下にTransformerのEncoder部分(4層)を書く(to do)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0545e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AcousticModelTransformer(nn.Module):\n",
    "    def __init__(self, classes_num, midfeat, momentum):\n",
    "        super(AcousticModelTransformer, self).__init__()\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=48, momentum=momentum)\n",
    "        self.conv_block2 = ConvBlock(in_channels=48, out_channels=64, momentum=momentum)\n",
    "        self.conv_block3 = ConvBlock(in_channels=64, out_channels=96, momentum=momentum)\n",
    "        self.conv_block4 = ConvBlock(in_channels=96, out_channels=128, momentum=momentum)\n",
    "\n",
    "        self.fc5 = nn.Linear(midfeat, 768, bias=False)\n",
    "        self.bn5 = nn.BatchNorm1d(768, momentum=momentum)\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(768)\n",
    "        self.pe = PositionalEncoding(d_model=768, dropout=0.1)\n",
    "\n",
    "        self.fc = nn.Linear(768, classes_num, bias=True)\n",
    "        \n",
    "        self.encoder_layer = TransformerEncoder()\n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_layer(self.fc5)\n",
    "        init_bn(self.bn5)\n",
    "        init_layer(self.fc)\n",
    "        init_ln(self.ln1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          input: (batch_size, channels_num, time_steps, freq_bins)\n",
    "        Outputs:\n",
    "          output: (batch_size, time_steps, classes_num)\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.conv_block1(input, pool_size=(1, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block2(x, pool_size=(1, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block3(x, pool_size=(1, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block4(x, pool_size=(1, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training) # [batch_size, 128, 1001, 14]\n",
    "        \n",
    "        x = x.transpose(1, 2).flatten(2)  # [batch_size, 1001, 1792]\n",
    "\n",
    "        x = F.relu(self.fc5(x)) # [batch_size, 1001, 768]\n",
    "        x = self.ln1(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training, inplace=True)  # [batch_size, 1001, 768]\n",
    "        x = self.pe(x) # [batch_soze, 1001, 768]\n",
    "        \n",
    "        x = self.encoder_layer(x)  # [batch_size, 1001, 768]\n",
    "        x = F.dropout(x, p=0.5, training=self.training, inplace=False)\n",
    "        \n",
    "        output = torch.sigmoid(self.fc(x))\n",
    "        \n",
    "        # この下にTransformerのEncoder部分(4層)を書く(to do)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8c6bb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Regress_onset_offset_frame_velocity_CRNN(nn.Module):\n",
    "    def __init__(self, frames_per_second, classes_num):\n",
    "        super(Regress_onset_offset_frame_velocity_CRNN, self).__init__()\n",
    "        \n",
    "        sample_rate = 16000\n",
    "        window_size = 2048\n",
    "        hop_size = sample_rate // frames_per_second\n",
    "        mel_bins = 229\n",
    "        fmin = 30\n",
    "        fmax = sample_rate // 2\n",
    "        \n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "        \n",
    "        midfeat = 1792\n",
    "        momentum = 0.01\n",
    "        \n",
    "        # Spectrogram\n",
    "        self.spectrogram_extractor = Spectrogram(n_fft=window_size,\n",
    "            hop_length=hop_size, win_length=window_size, window=window,\n",
    "            center=center, pad_mode=pad_mode, freeze_parameters=True)\n",
    "        \n",
    "        # Logmel\n",
    "        self.logmel_extractor = LogmelFilterBank(sr=sample_rate,\n",
    "            n_fft=window_size, n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref,\n",
    "            amin=amin, top_db=top_db, freeze_parameters=True)\n",
    "        \n",
    "        self.bn0 = nn.BatchNorm2d(mel_bins, momentum)\n",
    "        \n",
    "        self.frame_model = AcousticModelCRnn8Dropout(classes_num, midfeat, momentum)\n",
    "        self.reg_onset_model = AcousticModelCRnn8Dropout(classes_num, midfeat, momentum)\n",
    "        self.reg_offset_model = AcousticModelCRnn8Dropout(classes_num, midfeat, momentum)\n",
    "        self.velocity_model = AcousticModelTransformer(classes_num, midfeat, momentum)\n",
    "        \n",
    "        # after CRNN block\n",
    "        # only onset and frame is required\n",
    "        # \"attention\": figs (high_resolution and exploring transformer's pottential) is different but same model expect velocity network\n",
    "        self.reg_onset_gru = nn.GRU(input_size=88 * 2, hidden_size=256, num_layers=1,\n",
    "            bias=True, batch_first=True, dropout=0., bidirectional=True)\n",
    "        self.reg_onset_fc = nn.Linear(512, classes_num, bias=True)\n",
    "        \n",
    "        self.frame_gru = nn.GRU(input_size=88 * 3, hidden_size=256, num_layers=1,\n",
    "            bias=True, batch_first=True, dropout=0., bidirectional=True)\n",
    "        self.frame_fc = nn.Linear(512, classes_num, bias=True)\n",
    "        \n",
    "        self.init_weight()\n",
    "        \n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_gru(self.reg_onset_gru)\n",
    "        init_gru(self.frame_gru)\n",
    "        init_layer(self.reg_onset_fc)\n",
    "        init_layer(self.frame_fc)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input: (batch_size, data_length)\n",
    "        \n",
    "        Outputs:\n",
    "            output_dict: dict, {\n",
    "              'reg_onset_output': (batch_size, time_steps, classes_num),\n",
    "              'reg_offset_output': (batch_size, time_steps, classes_num),\n",
    "              'frame_output': (batch_size, time_steps, classes_num),\n",
    "              'velocity_output': (batch_size, time_steps, classes_num)\n",
    "            }\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.spectrogram_extractor(input) # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.logmel_extractor(x) # (batch_size, 1, time_step, freq_bins)\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        frame_output = self.frame_model(x)\n",
    "        reg_onset_output = self.reg_onset_model(x)\n",
    "        reg_offset_output = self.reg_offset_model(x)\n",
    "        velocity_output = self.velocity_model(x)\n",
    "        \n",
    "        # Concatenete veloacity and onset output to regress final onset output\n",
    "        x = torch.cat((reg_onset_output, (reg_onset_output ** 0.5) * velocity_output.detach()), dim=2)\n",
    "        (x, _) = self.reg_onset_gru(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training, inplace=False)\n",
    "        reg_onset_output = torch.sigmoid(self.reg_onset_fc(x))\n",
    "        \"\"\"(batch_size, time_steps, classes_num)\"\"\"\n",
    "        \n",
    "        # concatenate on/offset and frame outputs to classifier final pitch output\n",
    "        x = torch.cat((frame_output, reg_onset_output.detach(), reg_offset_output.detach()), dim=2)\n",
    "        (x, _) = self.frame_gru(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training, inplace=False)\n",
    "        frame_output = torch.sigmoid(self.frame_fc(x)) # (batch_size,  time_steps, classes_num)\n",
    "        \n",
    "        output_dict = {\n",
    "            'reg_onset_output': reg_onset_output,\n",
    "            'reg_offset_output': reg_offset_output,\n",
    "            'frame_output': frame_output,\n",
    "            'velocity_output': velocity_output}\n",
    "        \n",
    "        return output_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b30b7964",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_batch = torch.ones(10, 2000)\n",
    "NET = Regress_onset_offset_frame_velocity_CRNN(frames_per_second=100, classes_num = 88)\n",
    "outputs = NET(inputs_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4053826c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reg_onset_output': tensor([[[0.4472, 0.5248, 0.4360,  ..., 0.4015, 0.3873, 0.5748],\n",
       "          [0.3412, 0.5455, 0.5361,  ..., 0.3126, 0.4778, 0.4953],\n",
       "          [0.5900, 0.4921, 0.5005,  ..., 0.3069, 0.6806, 0.5425],\n",
       "          ...,\n",
       "          [0.6006, 0.6217, 0.5187,  ..., 0.3102, 0.3061, 0.4669],\n",
       "          [0.4405, 0.5557, 0.6065,  ..., 0.4622, 0.6525, 0.5339],\n",
       "          [0.6069, 0.6280, 0.5871,  ..., 0.6288, 0.5753, 0.3968]],\n",
       " \n",
       "         [[0.4576, 0.5362, 0.4016,  ..., 0.3953, 0.4700, 0.4444],\n",
       "          [0.5117, 0.4825, 0.4133,  ..., 0.3660, 0.5814, 0.5054],\n",
       "          [0.4855, 0.5966, 0.4856,  ..., 0.4008, 0.6052, 0.5014],\n",
       "          ...,\n",
       "          [0.5427, 0.3981, 0.3756,  ..., 0.4476, 0.5081, 0.3226],\n",
       "          [0.3589, 0.6175, 0.6390,  ..., 0.3305, 0.4635, 0.2567],\n",
       "          [0.4383, 0.5579, 0.4697,  ..., 0.5143, 0.5926, 0.4080]],\n",
       " \n",
       "         [[0.4521, 0.5502, 0.4157,  ..., 0.4810, 0.5748, 0.3728],\n",
       "          [0.5716, 0.5546, 0.4889,  ..., 0.3648, 0.6444, 0.2292],\n",
       "          [0.4312, 0.4575, 0.5321,  ..., 0.1505, 0.6824, 0.6552],\n",
       "          ...,\n",
       "          [0.5991, 0.4081, 0.5442,  ..., 0.4332, 0.4415, 0.6341],\n",
       "          [0.4697, 0.6313, 0.4478,  ..., 0.4196, 0.6320, 0.5861],\n",
       "          [0.5125, 0.5831, 0.2828,  ..., 0.2676, 0.5991, 0.4627]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.4799, 0.5385, 0.5040,  ..., 0.3271, 0.5717, 0.6145],\n",
       "          [0.5786, 0.5243, 0.4883,  ..., 0.3093, 0.5763, 0.3789],\n",
       "          [0.5885, 0.6301, 0.5638,  ..., 0.4363, 0.4946, 0.5283],\n",
       "          ...,\n",
       "          [0.6818, 0.5389, 0.6389,  ..., 0.3585, 0.4458, 0.6497],\n",
       "          [0.5668, 0.5133, 0.5903,  ..., 0.5780, 0.4412, 0.6488],\n",
       "          [0.5066, 0.5017, 0.7057,  ..., 0.5365, 0.6661, 0.6454]],\n",
       " \n",
       "         [[0.5053, 0.4349, 0.5704,  ..., 0.4607, 0.4769, 0.3945],\n",
       "          [0.5761, 0.5529, 0.3953,  ..., 0.2895, 0.6588, 0.4505],\n",
       "          [0.7624, 0.6525, 0.4723,  ..., 0.3942, 0.6028, 0.5076],\n",
       "          ...,\n",
       "          [0.5345, 0.5149, 0.7099,  ..., 0.4137, 0.5422, 0.6075],\n",
       "          [0.5089, 0.5821, 0.7375,  ..., 0.4178, 0.5015, 0.6353],\n",
       "          [0.6666, 0.5837, 0.6692,  ..., 0.5514, 0.6747, 0.4464]],\n",
       " \n",
       "         [[0.4431, 0.6585, 0.5979,  ..., 0.4766, 0.4652, 0.3670],\n",
       "          [0.5216, 0.5565, 0.6915,  ..., 0.2495, 0.4542, 0.5369],\n",
       "          [0.4864, 0.5024, 0.4872,  ..., 0.4392, 0.5990, 0.3715],\n",
       "          ...,\n",
       "          [0.5398, 0.5257, 0.3628,  ..., 0.3775, 0.5446, 0.5031],\n",
       "          [0.4786, 0.6812, 0.3293,  ..., 0.4632, 0.6080, 0.4938],\n",
       "          [0.5220, 0.4838, 0.5765,  ..., 0.4935, 0.5769, 0.5936]]],\n",
       "        grad_fn=<SigmoidBackward>),\n",
       " 'reg_offset_output': tensor([[[0.7390, 0.7085, 0.6831,  ..., 0.9222, 0.0433, 0.8983],\n",
       "          [0.2585, 0.5669, 0.9473,  ..., 0.9360, 0.2992, 0.9659],\n",
       "          [0.0595, 0.3116, 0.9332,  ..., 0.9866, 0.6857, 0.9939],\n",
       "          ...,\n",
       "          [0.0962, 0.6807, 0.8161,  ..., 0.8786, 0.8825, 0.9698],\n",
       "          [0.5134, 0.0856, 0.6207,  ..., 0.7196, 0.7191, 0.9429],\n",
       "          [0.2096, 0.4810, 0.9700,  ..., 0.7760, 0.8156, 0.3010]],\n",
       " \n",
       "         [[0.7294, 0.4463, 0.9421,  ..., 0.8401, 0.1813, 0.7923],\n",
       "          [0.4869, 0.8718, 0.9066,  ..., 0.9820, 0.8952, 0.8107],\n",
       "          [0.0607, 0.1682, 0.9029,  ..., 0.9454, 0.2438, 0.4806],\n",
       "          ...,\n",
       "          [0.1473, 0.5862, 0.4731,  ..., 0.8870, 0.7449, 0.5846],\n",
       "          [0.0867, 0.9338, 0.7463,  ..., 0.9254, 0.9225, 0.3129],\n",
       "          [0.4393, 0.9363, 0.5646,  ..., 0.9910, 0.6938, 0.9759]],\n",
       " \n",
       "         [[0.1433, 0.8407, 0.9200,  ..., 0.8119, 0.0187, 0.8886],\n",
       "          [0.8117, 0.1386, 0.9077,  ..., 0.6607, 0.3721, 0.9569],\n",
       "          [0.0716, 0.1566, 0.8528,  ..., 0.6613, 0.7441, 0.9649],\n",
       "          ...,\n",
       "          [0.3671, 0.8787, 0.5744,  ..., 0.9221, 0.5084, 0.7018],\n",
       "          [0.3114, 0.7375, 0.7186,  ..., 0.8390, 0.9222, 0.9800],\n",
       "          [0.6971, 0.5096, 0.9265,  ..., 0.8127, 0.8137, 0.5984]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.6269, 0.9251, 0.8563,  ..., 0.1681, 0.2031, 0.9272],\n",
       "          [0.5378, 0.7242, 0.9689,  ..., 0.1348, 0.1050, 0.7862],\n",
       "          [0.0192, 0.6795, 0.6620,  ..., 0.5751, 0.3057, 0.6351],\n",
       "          ...,\n",
       "          [0.5537, 0.7640, 0.6993,  ..., 0.7432, 0.0920, 0.8896],\n",
       "          [0.1258, 0.9117, 0.9489,  ..., 0.3233, 0.3878, 0.9327],\n",
       "          [0.9610, 0.8386, 0.8307,  ..., 0.1265, 0.1143, 0.8599]],\n",
       " \n",
       "         [[0.8738, 0.9563, 0.8794,  ..., 0.1493, 0.0934, 0.4340],\n",
       "          [0.3475, 0.8709, 0.9955,  ..., 0.6448, 0.3265, 0.8910],\n",
       "          [0.3911, 0.9817, 0.8316,  ..., 0.9593, 0.1485, 0.8154],\n",
       "          ...,\n",
       "          [0.8165, 0.1672, 0.4497,  ..., 0.7803, 0.0484, 0.7293],\n",
       "          [0.2256, 0.4207, 0.5853,  ..., 0.7457, 0.2254, 0.4516],\n",
       "          [0.2749, 0.7411, 0.7962,  ..., 0.5029, 0.1929, 0.6645]],\n",
       " \n",
       "         [[0.6067, 0.6958, 0.7732,  ..., 0.1573, 0.4258, 0.8539],\n",
       "          [0.0102, 0.9686, 0.8483,  ..., 0.5777, 0.5943, 0.2962],\n",
       "          [0.2304, 0.9709, 0.2054,  ..., 0.1217, 0.3629, 0.8644],\n",
       "          ...,\n",
       "          [0.3065, 0.7848, 0.5023,  ..., 0.5472, 0.3268, 0.2943],\n",
       "          [0.2055, 0.8699, 0.9208,  ..., 0.2035, 0.3912, 0.2925],\n",
       "          [0.0169, 0.9551, 0.7808,  ..., 0.6785, 0.1204, 0.8903]]],\n",
       "        grad_fn=<SigmoidBackward>),\n",
       " 'frame_output': tensor([[[0.3915, 0.4901, 0.5781,  ..., 0.6933, 0.6218, 0.4322],\n",
       "          [0.3677, 0.6342, 0.5430,  ..., 0.7265, 0.4810, 0.1988],\n",
       "          [0.3881, 0.5153, 0.8420,  ..., 0.7203, 0.3937, 0.3603],\n",
       "          ...,\n",
       "          [0.2672, 0.6225, 0.6468,  ..., 0.8016, 0.3805, 0.3627],\n",
       "          [0.2497, 0.5243, 0.5668,  ..., 0.6991, 0.5082, 0.2844],\n",
       "          [0.1518, 0.5065, 0.6535,  ..., 0.8500, 0.3763, 0.3701]],\n",
       " \n",
       "         [[0.4988, 0.6230, 0.5017,  ..., 0.5836, 0.4447, 0.4718],\n",
       "          [0.1774, 0.5960, 0.3866,  ..., 0.7321, 0.4944, 0.2342],\n",
       "          [0.2886, 0.5369, 0.6947,  ..., 0.7944, 0.3846, 0.3617],\n",
       "          ...,\n",
       "          [0.0676, 0.7265, 0.7305,  ..., 0.6199, 0.5696, 0.2085],\n",
       "          [0.3886, 0.5444, 0.7110,  ..., 0.6111, 0.5149, 0.2772],\n",
       "          [0.2943, 0.5096, 0.5986,  ..., 0.7008, 0.5230, 0.3501]],\n",
       " \n",
       "         [[0.2955, 0.5906, 0.5707,  ..., 0.6452, 0.5754, 0.3600],\n",
       "          [0.3291, 0.5916, 0.6201,  ..., 0.7149, 0.4918, 0.2611],\n",
       "          [0.1444, 0.4460, 0.5228,  ..., 0.7398, 0.5626, 0.3668],\n",
       "          ...,\n",
       "          [0.1104, 0.4625, 0.7091,  ..., 0.7139, 0.4322, 0.2107],\n",
       "          [0.2313, 0.7170, 0.6643,  ..., 0.7095, 0.5191, 0.2207],\n",
       "          [0.1906, 0.4750, 0.7245,  ..., 0.6369, 0.7802, 0.4935]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.1325, 0.5759, 0.6949,  ..., 0.4545, 0.5124, 0.4314],\n",
       "          [0.2170, 0.4834, 0.7255,  ..., 0.7747, 0.5350, 0.2552],\n",
       "          [0.2618, 0.6848, 0.4074,  ..., 0.6972, 0.5617, 0.4733],\n",
       "          ...,\n",
       "          [0.0739, 0.3473, 0.5289,  ..., 0.6939, 0.3604, 0.3213],\n",
       "          [0.2052, 0.5175, 0.6718,  ..., 0.8143, 0.5088, 0.5670],\n",
       "          [0.2763, 0.4706, 0.6719,  ..., 0.7133, 0.2791, 0.3877]],\n",
       " \n",
       "         [[0.3653, 0.4459, 0.5694,  ..., 0.6202, 0.3978, 0.4568],\n",
       "          [0.3430, 0.6519, 0.6155,  ..., 0.7798, 0.4606, 0.5330],\n",
       "          [0.2329, 0.5616, 0.6308,  ..., 0.7158, 0.3551, 0.4528],\n",
       "          ...,\n",
       "          [0.2336, 0.5553, 0.5315,  ..., 0.7023, 0.8133, 0.4525],\n",
       "          [0.1290, 0.5176, 0.7041,  ..., 0.7608, 0.5390, 0.3366],\n",
       "          [0.4075, 0.2884, 0.7136,  ..., 0.8045, 0.5419, 0.2656]],\n",
       " \n",
       "         [[0.2267, 0.6163, 0.6103,  ..., 0.6651, 0.4543, 0.4553],\n",
       "          [0.2561, 0.5513, 0.5058,  ..., 0.6341, 0.4859, 0.3919],\n",
       "          [0.2104, 0.7674, 0.5819,  ..., 0.7294, 0.3965, 0.2963],\n",
       "          ...,\n",
       "          [0.3808, 0.6073, 0.7008,  ..., 0.8240, 0.4583, 0.3853],\n",
       "          [0.4856, 0.5139, 0.6166,  ..., 0.7233, 0.5981, 0.5685],\n",
       "          [0.2068, 0.5774, 0.6286,  ..., 0.7022, 0.4506, 0.5560]]],\n",
       "        grad_fn=<SigmoidBackward>),\n",
       " 'velocity_output': tensor([[[0.1134, 0.1991, 0.0697,  ..., 0.5431, 0.8605, 0.7257],\n",
       "          [0.6275, 0.3197, 0.3794,  ..., 0.8170, 0.4084, 0.1057],\n",
       "          [0.2785, 0.1114, 0.0733,  ..., 0.7371, 0.3690, 0.4467],\n",
       "          ...,\n",
       "          [0.5043, 0.3011, 0.5409,  ..., 0.3021, 0.3953, 0.5985],\n",
       "          [0.5058, 0.6558, 0.0786,  ..., 0.5975, 0.7294, 0.4929],\n",
       "          [0.5486, 0.1522, 0.3660,  ..., 0.3737, 0.3399, 0.0945]],\n",
       " \n",
       "         [[0.8440, 0.9259, 0.2920,  ..., 0.2669, 0.7697, 0.1546],\n",
       "          [0.4177, 0.0394, 0.3564,  ..., 0.5927, 0.3679, 0.4664],\n",
       "          [0.5155, 0.2734, 0.0113,  ..., 0.6180, 0.4214, 0.1919],\n",
       "          ...,\n",
       "          [0.2961, 0.5255, 0.0480,  ..., 0.8175, 0.5744, 0.1269],\n",
       "          [0.5649, 0.1085, 0.0497,  ..., 0.7259, 0.1916, 0.0673],\n",
       "          [0.3198, 0.8487, 0.0997,  ..., 0.9494, 0.3149, 0.2781]],\n",
       " \n",
       "         [[0.9790, 0.3229, 0.3915,  ..., 0.9409, 0.6098, 0.5039],\n",
       "          [0.4216, 0.7798, 0.0902,  ..., 0.3156, 0.7335, 0.3621],\n",
       "          [0.8862, 0.0878, 0.1352,  ..., 0.5461, 0.5535, 0.8237],\n",
       "          ...,\n",
       "          [0.1657, 0.1329, 0.0346,  ..., 0.2581, 0.2430, 0.2270],\n",
       "          [0.6347, 0.1981, 0.2253,  ..., 0.7412, 0.1388, 0.7728],\n",
       "          [0.2114, 0.0656, 0.1215,  ..., 0.2544, 0.6775, 0.1543]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.0393, 0.7907, 0.0285,  ..., 0.9566, 0.4930, 0.5993],\n",
       "          [0.6165, 0.4169, 0.0711,  ..., 0.5304, 0.5508, 0.8662],\n",
       "          [0.2307, 0.2257, 0.5695,  ..., 0.4051, 0.0784, 0.5312],\n",
       "          ...,\n",
       "          [0.6167, 0.8027, 0.3116,  ..., 0.2284, 0.4581, 0.6233],\n",
       "          [0.2892, 0.6361, 0.1838,  ..., 0.9350, 0.5803, 0.3243],\n",
       "          [0.1523, 0.1198, 0.0117,  ..., 0.8469, 0.9019, 0.6522]],\n",
       " \n",
       "         [[0.0681, 0.9261, 0.3456,  ..., 0.7701, 0.3935, 0.1078],\n",
       "          [0.0813, 0.8182, 0.0061,  ..., 0.7563, 0.1127, 0.0749],\n",
       "          [0.1103, 0.4405, 0.1203,  ..., 0.3855, 0.5116, 0.0878],\n",
       "          ...,\n",
       "          [0.8455, 0.8979, 0.1502,  ..., 0.2707, 0.4466, 0.7438],\n",
       "          [0.7175, 0.8179, 0.5105,  ..., 0.7415, 0.3411, 0.5061],\n",
       "          [0.4605, 0.6137, 0.0541,  ..., 0.9711, 0.2412, 0.4686]],\n",
       " \n",
       "         [[0.0679, 0.7681, 0.0047,  ..., 0.3047, 0.1438, 0.3434],\n",
       "          [0.3816, 0.8435, 0.0351,  ..., 0.9271, 0.0988, 0.7740],\n",
       "          [0.3333, 0.3867, 0.1758,  ..., 0.7645, 0.1728, 0.6760],\n",
       "          ...,\n",
       "          [0.7525, 0.4200, 0.0107,  ..., 0.7317, 0.6651, 0.1292],\n",
       "          [0.3564, 0.6987, 0.1714,  ..., 0.8167, 0.4104, 0.6532],\n",
       "          [0.1736, 0.1988, 0.0519,  ..., 0.8836, 0.1740, 0.2695]]],\n",
       "        grad_fn=<SigmoidBackward>)}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a0a34c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
